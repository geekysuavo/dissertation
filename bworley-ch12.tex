
\chapter{Summary and Future Directions}

\begin{quote}
{\it
  Chemists, in particular, cannot understand why they should fund
  someone to do data analysis.}
\\\\
 -- Richard Brereton \cite{brereton:jchemo2014b}
\end{quote}

\section{The Need for Data Handling}

\begin{doublespace}
The field of chemometrics is still in its infancy, but the chemometric
practice of extracting quantitative chemical information from data collected
on complex samples is much older, and has innumerable applications in
chemistry \cite{wold:cils1995,brereton:jchemo2014b}. While the standard
toolbox of $t$-tests, run charts and univariate distributions has served
analytical chemists well, the analysis of spectral measurements of
multi-component mixtures demands a more computationally intensive
approach.
\\\\
However, optimal chemometric modeling of spectral data does not begin when
the data are read in for the first time, but before acquisition has even
been performed. Successful experimental design relies on data collection
procedures that yield informative, high-quality measurement results. Spectra
having the highest possible resolution, dynamic range and signal-to-noise
ratio are necessary if reliable conclusions are to be drawn from their models.
In multidimensional NMR experiments, methods of sparse data collection are
becoming increasingly popular, as they provide avenues for maximizing spectral
quality. In these nonuniform sampling (NUS) methods, the greatest contributing
factor to spectral quality is the sampling scheme, and the generation of
sampling schemes that optimize various spectral features (i.e. sensitivity
or resolution) is still an active area of fundamental research
\cite{mobli:jmr2015}. \hyperlink{chapter.2}{Chapter 2} introduces a general
framework for multidimensional nonuniform sampling that extends the work of
Hyberts and Wagner \cite{hyberts:jacs2010} and deterministically generates
nonuniform sampling schedules that perform as well or better than stochastic
methods \cite{worley:jmr2015}. By suggesting an alternative mechanism for
introducing irregularity into a sampling schedule, burst-augmented gap
sampling aims to provoke further investigation into which features of a
sampling schedule yield optimal spectral results. Furthermore, this new
framework is the first proposed mechanism for deterministically constructing
sampling schedules on a multidimensional Nyquist grid \cite{eddy:jmr2012}
based on a general equation.
\\\\
The processing, treatment and modeling of spectral measurements using
multivariate statistics, outlined in \hyperlink{chapter.3}{Chapter 3},
is a nuanced task, with many pitfalls awaiting the chemist who lacks
experience and training in multivariate data analysis
\cite{worley:cmb2013,worley:anchem2015}. Most applications of chemometrics
are performed by analytical chemists, whose expertise lies with a certain
type of instrumentation rather than statistics. In order to promote
statistically sound data handling practices, chemometricians
must begin to place easy-to-use, well-documented software packages in the
hands of chemists. These software packages must simultaneously provide
powerful mechanisms of multivariate data analysis, educate users about
proper data handling, and encourage further extension and collaboration
between fundamentally focused chemometricians and applications-driven
chemists. \hyperlink{chapter.5}{Chapter 5} introduces MVAPACK
\cite{worley:acscb2014}, an open-source suite of simple GNU Octave
\cite{eaton2008} functions that aims to address those goals, and
\hyperlink{chapter.4}{Chapter 4} describes its use on a wide variety of
applications within the rapidly growing field of metabolomics
\cite{worley:acscb2014,marshall:metab2015,worley:anchem2015}. The release
of MVAPACK under an open-source license ensures transparency, allows for
critical review by expert members of the chemometrics community, and
enables modification and extension by its user base.
\\\\
Without a doubt, the availability of MVAPACK made the development of
phase-scatter correction
(PSC, \hyperlink{chapter.6}{Chapter 6}),
uncomplicated statistical spectral remodeling
(USSR, \hyperlink{chapter.7}{Chapter 7}),
and generalized adaptive intelligent binning
(GAI-binning, \hyperlink{chapter.8}{Chapter 8})
substantially easier
\cite{worley:cils2014,worley:jbnmr2015,worley:cils2015}. By interlacing
these methods into the existing fabric of MVAPACK, only a single new
function had to be written for each new method. The data structures
required by the algorithms -- complex data matrices, real data matrices,
and arrays of real matrices -- are provided in a well-defined format by
existing functions in MVAPACK. As a result, development could be focused
100\% on functionalities of the \emph{actual algorithms}, and not the
``glue code'' typically required to make a method even moderately useful.
This modularity conveys distinct advantages to the entire community:
from the perspective of an MVAPACK user, adding PSC or GAI-binning into
an existing data handling protocol requires changing a single function
call in an Octave script.\footnote{After upgrading to the latest version
of MVAPACK, of course.}
\\\\
As more chemists begin to collect multiple analytical measurements from each
sample, it is imperative that well-defined, statistically acceptable methods
be available to model the resulting data \cite{westerhuis:jchemo1997,
  westerhuis:jchemo1998,smilde:jchemo2003,
  marshall:metab2015,worley:jchemo2015}. Consensus PCA (CPCA-W) and Multiblock
PLS (MB-PLS), discussed in \hyperlink{chapter.3}{Chapter 3}, are powerful
extensions of PCA and PLS modeling to multiblock datasets, and are implemented
in MVAPACK to provide easy access by the community. However, at the time of
their implementation, no analogous extension of OPLS existed to handle
multiblock data. Most chemists using multivariate statistics prefer OPLS
over PLS due to its enhanced
interpretability \cite{trygg:jchemo2002,tapp:trac2009}, and the lack of
a formally defined multiblock analog of OPLS was starting to become apparent
in the form of several ad hoc attempts to use single-block OPLS on multiple
matrices \cite{bylesjo:jpr2009,boccard:aca2013}.
\hyperlink{chapter.9}{Chapter 9} formally defines Multiblock OPLS (MB-OPLS)
as a consensus bilinear factorization method, relates it to the OnPLS method
proposed by L\"{o}fstedt and Trygg \cite{lofstedt:jchemo2011}, and describes
its relationship to several other methods (CPCA-W, MB-PLS, nPLS and OnPLS)
in the context of the highly general framework described by Hanafi and Kiers
\cite{hanafi:csda2006}. Coupled with the inclusion of MB-OPLS in MVAPACK,
this description presents a thoroughly vetted avenue for chemists to easily
model their multiblock data using OPLS, without resorting to ad hoc
approaches \cite{worley:jchemo2015}.
\\\\
However, bilinear factorizations like PCA, PLS and OPLS merely represent
\emph{the very first step} towards chemometric modeling of spectral data
\cite{gromski:aca2015}. Ideal chemometric models of spectral measurements
would directly report the concentrations of the individual components in
the mixtures being studied \cite{eads:anchem2004}. In the context of bilinear
modeling, achieving this goal requires the imposition of stronger constraints
on the model scores and loadings (cf. \hyperlink{section.3.5}{Section 3.5}).
Methods such as multivariate curve resolution by alternating least squares
(MCR-ALS, \cite{dejuan:crac2006}) and molecular factor analysis
(MFR, \cite{eads:anchem2004}) impose non-negativity constraints on both
the scores and loadings in an alternating least squares framework, while
bayesian spectral decomposition (BSD, \cite{ochs:jmr1999,stoyanova:anchem2004})
and bayesian positive source separation (BPSS, \cite{moussaoui:ieee2006,
  moussaoui:cils2006}) do so by assigning prior
probabilities to their values. While these methods report more chemically
and spectroscopically relevant information than PCA by imposing non-negativity,
they still return mixtures of multiple compounds in their loadings. Imposition
of ``hard modeling'' constraints takes the problem a step further by requiring
each extracted signal to obey a certain parametric form. Hard modeling of NMR
data has been accomplished using time-domain Bayesian
\cite{bretthorst:jmr1990a,bretthorst:jmr1990b,bretthorst:jmr1990c,
      chylla:jbnmr1993} and maximum-likelihood \cite{chylla:jbnmr1995}
modeling, as well as hybrid time- and frequency-domain maximum-likelihood
\cite{chylla:jbnmr1998,chylla:anchem2011,hu:anchem2011} modeling. Such
methods translate the task of identifying mixture components into one of
peak-matching, where each signal in the model is assigned to a known set
of signals from a given compound. Inclusion of compound identity information
in the modeling process is the final step towards directly decomposing complex
mixtures into their substituent parts. Methods such as BATMAN
\cite{astle:jasa2012,hao:binf2012} and BQuant \cite{zheng:binf2011} have
been shown to perform quite capably in modeling 1D \hnmr{} NMR spectra, but
require the specification of spectral information for each potential mixture
component, and tend to be computationally expensive. Despite the clear
advantage these more complex approaches hold over soft modeling, their
adoption by the chemistry community has proceeded incredibly slowly. Without
easy-to-use software implementations, these advanced modeling algorithms are
likely to remain a mere afterthought in applied fields like metabolomics,
where usability often outweighs capability.
\\\\
Once models have been constructed around a dataset, the task of inference
begins, usually involving a great deal more expert chemical or biochemical
insight than model construction required. At this stage, it can be tempting
to draw conclusions based on patterns observed in the results. The tendency
of human perception to over-fit patterns to data can, however, lead analysts
to infer too much from a model. When chemical conclusions must be drawn from
scores plots of PCA, PLS-DA or OPLS-DA models, there are simple statistical
measures that can be taken to avoid misperception of scores-space patterns.
\hyperlink{chapter.10}{Chapter 10} introduces a set of utilities to
quantitatively measure and depict separations between classes in
model scores \cite{worley:abio2013}. These utilities are based on the
Mahalanobis distance, a multivariate meterstick \cite{demaesschalck:cils2000}
that takes distributional properties of each class into account. Using these
utilities, analysts may confidently make statistical arguments about distances
between experimental classes in a dataset.
\\\\
Finally, the possibility of the existence a new fundamental electronic
interaction (the \npistar{} interaction) in proteins was explored in
\hyperlink{chapter.11}{Chapter 11}. Given the massive amounts of chemical
and biological data available in depositories like the protein databank
(PDB, \cite{berman:nar2000}) and the biological magnetic resonance bank
(BMRB, \cite{ulrich:nar2008}), cheminformatic and bioinformatic data mining
efforts such as the one performed in \hyperlink{chapter.11}{Chapter 11} are
becoming increasingly possible. Efforts to curate the existing wealth of data
into usable quantities, such as models of protein \hnmr{}, \cnmr{} and \nnmr{}
chemical shifts \cite{osapay:jacs1991,spera:jacs1991,iwadate:jbnmr1999,
  han:jbnmr2011,li:jmr2015} and order parameters
\cite{berjanskii:jacs2005,berjanskii:jbnmr2008} have proven useful in
protein structure determination and refinement protocols. Combined with
high-accuracy distance \cite{vogeli:pnmrs2014} and orientation
\cite{li:cpc2015} restraints, these databases will increasingly serve
as sources of prior information in novel probabilistically driven structure
determination efforts \cite{nilges:struct2009,olsson:jctc2014}.
\\\\
While the raw amount of data is not quite as overwhelming in chemometrics as
it may be in cheminformatic and bioinformatic studies, its complexity is just
as great. As efforts to model protein structure and dynamics, cellular
metabolic flux, and multicomponent chemical mixtures continue, data handling
methods must be advanced to ensure maximal ``information handling'' is
achieved. These methods must be provided in easy-to-use software packages
that allow their users to focus on scientific inquiry, rather than trudging
though manual pages to find the special syntax of a given function. Of course,
doing so will require deep, multidisciplinary collaboration between groups
that specialize in computer science, mathematics, statistics, chemistry and
biology.
\end{doublespace}

\bibliographystyle{abbrv}
\bibliography{bworley}

