
\chapter{Phase-Scatter Correction of NMR Datasets}

\section{Introduction}

\begin{doublespace}
Normalization applied directly to hypercomplex NMR data (or its real component)
is sub-optimal, as even small phase differences between observations can
frustrate the estimation of normalization factors
(See \hyperlink{section.3.3}{Section 3.3}). Possibly worse, blind
normalization of poorly phased spectral data can accentuate experimentally
irrelevant spectral features in a data tensor during multivariate modeling,
leading the analyst to erroneous conclusions. These difficulties motivated
the development of phase-scatter correction (PSC, \cite{worley:cils2014}) as
a means of simultaneously correcting the coupled phase errors and dilution
errors that are present in hypercomplex NMR data tensors.
\end{doublespace}

\subsection{Metabolomics}

\begin{doublespace}
As previously introduced in \hyperlink{chapter.3}{Chapter 3}, normalization
of data tensors is a commonly performed procedure aimed at minimizing the
within-class variation of two or more groups of observations, relative to
the total or between-class variation in the dataset. Irrespective of whether
separations between classes are obtained using an unsupervised PCA model or a
supervised (O)PLS-DA model, greater statistical significance and increased
biological relevance may be ascribed to separations between classes having
greater variation between groups than within them \cite{worley:abio2013}.
When hypercomplex NMR data must be normalized prior to multivariate analyses
within the confines of a metabolomics study, the interrelation of phase and
dilution errors is best handled using phase-scatter correction.
\end{doublespace}

\subsection{High-throughput Screening}

\begin{doublespace}
A second application of phase-scatter correction exists in the form of NMR
protein-ligand affinity screening. NMR spectroscopy reports a multitude of
time-averaged physical observables that carry information relating to the
nature of interactions between small molecule ligands and protein targets
\cite{lepre:chemrev2004}. A number of 1D \hnmr{} NMR pulse sequences have
been developed to probe these distinct features of binding, including
differences in free and bound ligand diffusion and relaxation properties
\cite{hajduk:jacs1997}, and saturation transfers from water
\cite{dalvit:jbnmr2000} and protein \cite{mayer:jacs2001} resonances. As part
of an NMR high-throughput screen, these 1D \hnmr{} NMR pulse sequences present
a number of unique challenges that include high false positive rates, long
acquisition times, and high demand for protein samples
\cite{lepre:menz2011,harner:jbnmr2013}. However, at suitably chosen
concentrations of ligand and protein, a standard, unedited 1D \hnmr{} NMR
experiment may be used to detect binding interactions through enhanced
relaxation rates of ligand spins
\cite{mercier:jacs2006,powers:ddt2008,mercier:cchts2009}.
\\\\
While it is possible to detect ligand binding using standard 1D \hnmr{} NMR,
the resulting spectra are a combination of free and bound ligand and protein
signals, a fact which makes them difficult to interpret. Broad, rolling
baselines arising from slowly tumbling protein spins are particularly
problematic during interpretation, as they often mask changes in ligand signal
broadness and intensity. This masking effect due to protein baselines is
exacerbated at protein-ligand concentration ratios nearing or exceeding unity,
forcing the use of excess ligand and increasing the false negative rate during
screening. To mitigate these issues, a statistical method called Uncomplicated
Statistical Spectral Remodeling (USSR), was developed that removes protein
baselines from high-throughput ligand-based screening datasets by leveraging
inter-sample reproducibility of protein signals. In addition, it will be
demonstrated that the use of phase-scatter correction greatly improves
inter-sample protein baseline reproducibility and reduces the false-positive
rate incurred by subsequent USSR-based analyses. The combination of PSC and
USSR enables a rapid analysis of standard 1D \hnmr{} NMR screening data,
especially in difficult cases having a high protein-ligand concentration ratio.
\end{doublespace}

\section{Theory}

\subsection{Multiplicative Scatter Correction}

\begin{doublespace}
Phase-scatter correction (PSC) is effectively an extension of multiplicative
scatter correction (MSC) to handle phase errors during normalization. In MSC,
each real spectrum is scaled around its mean intensity and shifted to match a
reference spectrum, typically the mean of the dataset \cite{fearn:cils2009}.
Optimal normalization factors ($\mathbf{b}$) of a data matrix $\mathbf{X}$ are
determined by a linear regression of the mean-centered reference vector onto
the mean-centered matrix:

\begin{equation}
\big( \mathbf{X} - \langle \mathbf{X} \rangle \big)^T \mathbf{b}
 = \big( \mathbf{r} - \langle \mathbf{r} \rangle \big)^T
\end{equation}

where observations are stored as row vectors in $\mathbf{X}$, and $\mathbf{r}$
is the reference observation row vector. The equation above represents an
overdetermined system of linear equations, therefore the least-squares estimate
of $\mathbf{b}$ may be computed rapidly, and MSC is rather computationally
efficient.
\end{doublespace}

\subsection{Phase-scatter Correction}

\begin{doublespace}
PSC additionally corrects zero- and first-order phase errors during
normalization, requiring a nonlinear optimization of the following objective:

\begin{equation}
Q(\mathbf{X} \mid \mathbf{p})
 = \sum_{n=1}^N \| \mathbf{z}_n \circ \mathbf{x}_n - \mathbf{r} \|_2^2
\end{equation}

where $\circ$ denotes the element-wise product, the mean-centered matrix
$\mathbf{X}$ lies in $\mathbb{H}_1^{N \times K}$, the mean-centered reference
$\mathbf{r}$ lies in $\mathbb{H}_1^K$, and the set of parameters $\mathbf{p}$
includes a normalization factor and two phase errors per observation
in $\mathbf{X}$:

\begin{equation}
\mathbf{p} = \{
  b_1, \dots b_N,
  \theta_{0,1}, \dots \theta_{0,N},
  \theta_{1,1}, \dots \theta_{1,N} \}
\end{equation}

and each vector $\mathbf{z}_n$ contains the normalization and phase corrections
for the $n$-th observation $\mathbf{x}_n$:

\begin{equation}
z_{n,k} = b_n e^{u_1 (\theta_{0,n} + \theta_{1,n} k)}
\end{equation}

Because the reference observation $\mathbf{r}$ is fixed during optimization,
minimization of $Q(\mathbf{X} \mid \mathbf{p})$ may be achieved by
independently minimizing each observation's contributions. Minimization is
carried out for every observation in the data matrix using Levenberg-Marquardt
nonlinear least squares \cite{marquardt:jsiam1963} as implemented by the
{\it leasqr} function in GNU Octave, a function similar to MATLAB's
{\it nlinfit}. Each corrected spectrum $\hat{\mathbf{x}}_n$ is then returned
from optimization as follows:

\begin{equation}
\hat{\mathbf{x}}_n
 = \mathbf{z}_n \circ \mathbf{x}_n
 + \langle \mathbf{r} \rangle
\end{equation}

Phase-scatter correction of 50 1D \hnmr{} NMR spectra having 32$k$ complex
points each requires approximately 30 seconds on a single-core 3.2 GHz Intel
workstation running GNU Octave 3.6.
\end{doublespace}

\subsection{Ensemble Phase Correction}

\begin{doublespace}
It is important recognize that the phase-scatter correction objective function
$Q(\mathbf{X} \mid \mathbf{p})$ provides no measure of ideal phase values,
meaning that PSC requires an additional phase correction step prior to its
execution in order to ensure adequate initial conditions. Even when
$\mathbf{X}$ has been suitably phase-corrected, PSC may still attempt to
minimize scatter between spectra by re-introducing phase errors. This
undesirable behavior of PSC may be observed when large disparities in
spectral intensities are present between observations. To correct this,
a standard phase correction objective
$f : \mathbb{H}_D^{K} \to \mathbb{R}$ may be combined with
the PSC objective using a Lagrange multiplier, like so:

\begin{equation}
\Lambda(\mathbf{X} \mid \mathbf{p}) =
 -\sum_{n=1}^N f(\boldsymbol{\theta}_n \circ \mathbf{x}_n) +
 \lambda \sum_{n=1}^N \| \mathbf{z}_n \circ \mathbf{x}_n -
            \langle \mathbf{Z} \circ \mathbf{X} \rangle \|_2^2
\end{equation}

where the correction matrix $\mathbf{Z}$ has the same form as in PSC, expressed
as a real diagonal matrix of normalization factors $\mathbf{B}$ and a
hypercomplex matrix of phase factors $\mathbf{\Theta}$:

\begin{equation}
\mathbf{Z} = \mathbf{B} \mathbf{\Theta}
\end{equation}

and $\boldsymbol{\theta}_n$ is the $n$-th row of $\mathbf{\Theta}$. The new
ensemble phase correction (EPC) objective function
$\Lambda(\mathbf{X} \mid \mathbf{p})$ balances the potentially opposing goals
of phase correction and scatter correction through the Lagrange multiplier
$\lambda$, and does not require the specification of a reference observation
$\mathbf{r}$. In effect, EPC allows its scatter correction reference to float
as the current mean of the data, $\langle \mathbf{Z} \circ \mathbf{X} \rangle$.
This floating reference requires the simultaneous optimization of all the
parameters in $\mathbf{p}$, unlike phase-scatter correction. Efficient
minimization of $\Lambda(\mathbf{X} \mid \mathbf{p})$ may be accomplished by
a modified Nelder-Mead simplex optimization procedure \cite{nelder:compj1964},
which serially updates the simplices of all observations at each global
iteration and maintains the current mean vector
$\langle \mathbf{Z} \circ \mathbf{X} \rangle$ at each update.
\\\\
In contrast to phase-scatter correction, which seeks to minimize the scatter
of data matrix observations around a fixed reference, ensemble phase correction
approaches the dilemma of entwined phase and normalization errors from an
opposing direction by introducing a scatter term into a standard automatic
phase correction procedure. The amount of normalization achieved by EPC is
directly controlled by the magnitude of $\lambda$: in the opposite limits of
$\lambda = 0$ and $\lambda \to \infty$, EPC becomes equivalent to standard
phase correction and phase-scatter correction with a floating reference,
respectively.
\end{doublespace}

\section{Materials and Methods}

\subsection{Metabolomics}

\begin{SCfigure}
\includegraphics[width=3.5in]{figs/pscorr/01-minj2.png}
\caption
      [Cluster Quality after Normalization and PCA Modeling.]{
  {\bf Cluster Quality after Normalization and PCA Modeling.}
  \\
  Comparison of PCA cluster quality for \hnmr{} NMR metabolomics data
  normalized using different algorithms. The minimum $J_2$ value (worst
  cluster quality) for each model is reported here, as it is a more effective
  indicator of overall model and cluster quality than the mean or median.
}
\end{SCfigure}

\subsubsection{NMR Data Processing}

\begin{doublespace}
Previously collected \hnmr{} NMR spectral data from published work
\cite{halouska:acscb2012} was leveraged as a typical metabolomics dataset
for performance analysis of PSC versus other normalization methods. Free
induction decays were loaded into GNU Octave 3.6 \cite{eaton2008} for
processing using MVAPACK routines \cite{worley:acscb2014}. Time-domain signals
were zero-filled to 32$k$ points and Fourier transformed, resulting in
a complex data matrix of 177 spectra divided amongst 16 classes
($N = 177, K = 32768, M = 16$). Spectra were both automatically phase corrected
by simplex entropy minimization \cite{chen:jmr2002} and manually phase
corrected by applying a constant phase shift to all spectra. Both automatically
and manually phase corrected datasets were then normalized using the CS, PQ,
HM, SNV, MSC and PSC methods (cf. \hyperlink{subsection.3.4.3}{Normalization}).
Each normalized data matrix was binned using a uniform 0.04 ppm bin width,
scaled per-variable to unit variance, and subjected to PCA. The $J_2$ statistic
\cite{koutroumbas2006} was calculated for each class to provide a measure of
cluster quality for the PCA scores from each normalization method, as follows:
\begin{equation}
J_{2,m} = \frac{|\mathbf{C}|}{|\mathbf{C}_m|}
\end{equation}
where $\mathbf{C}_m$ is the covariance matrix of the scores in class $m$,
$\mathbf{C}$ is the covariance matrix of all scores, and the vertical bars
represent the matrix determinant. Thus, as a cluster shrinks relative to the
entirety of the scores-space data, its $J_2$ statistic will increase. While
$J_2$ provides a measure of individual cluster tightness, it does not capture
the degree of cluster overlap within a dataset. Figures 6.1 and 6.2 show the
results of the $J_2$ calculation for normalization methods applied to real
\hnmr{} NMR metabolomics data.
\end{doublespace}

\begin{SCfigure}
\includegraphics[width=3.5in]{figs/pscorr/02-allj2.png}
\caption
      [Cluster Quality after Normalization and PCA Modeling.]{
  {\bf Cluster Quality after Normalization and PCA Modeling.}
  \\
  Comparison of PCA cluster quality for \hnmr{} NMR metabolomics data
  normalized using different algorithms. For each normalization method, a
  box is defined by the estimated first and third quartiles of $J_2$ for the
  clusters and whiskers are defined by the range of the $J_2$ values for the
  clusters. For this dataset, the minimum $J_2$ value is most instructive,
  given the fact that overall model quality is not well-reflected by the
  $J_2$ metric in the case of distorted principal components.
}
\end{SCfigure}

\begin{doublespace}
To quantify differences between extracted principal components of automatically
and manually phase corrected datasets, the angle between the first principal
component loading vector of each pair of models ($\varphi$) was calculated as
follows:
\begin{equation}
\varphi = \cos^{-1}\left( {\mathbf{p}_{auto}}^T \mathbf{p}_{man} \right)
\end{equation}
where $\mathbf{p}_{auto}$ and $\mathbf{p}_{man}$ are the first-component
loadings computed from a given normalization method's data after automatic
and manual phase correction, respectively. The loading angle $\varphi$ for a
given normalization method is a reflection on that method's ability to properly
normalize data and produce consistent PCA models from different initial phase
error conditions.
\end{doublespace}

\begin{table}[h!]
\caption{Metabolite Spectra Used in Monte Carlo Simulations.}
\begin{center}
\begin{tabular}{l l l l}
  \hline
  Aminobutyrate & Adenosine & Alanine     & Arginine   \\
  Asparagine    & Aspartate & Choline     & Citrulline \\
  Ethanolamine  & Fructose  & Galactose   & Glucose    \\
  Glutamate     & Glutamine & Glycine     & Histidine  \\
  Isoleucine    & Lactate   & Leucine     & Lysine     \\
  Malate        & Maltose   & Myoinositol & Ornithine  \\
  Phenylalanine & Proline   & Putrescine  & Serine     \\
  Succinate     & Sucrose   & Threonine   & Valine
\end{tabular}
\end{center}
\end{table}

\subsubsection{Simulated NMR Datasets}

\begin{doublespace}
The \hnmr{} NMR spectra of 100 mM samples of 32 metabolites (Table 6.1) at
pH 7.4 were downloaded from the Biological Magnetic Resonance Bank
(BMRB, \cite{ulrich:nar2008}) and fit to mixtures of complex Lorentzian
functions using ACD/1D NMR Processing (Advanced Chemistry Development).
Peak amplitudes ($A$), chemical shifts ($\omega_0$), and widths ($\lambda$)
returned from fitting were loaded into GNU Octave to generate simulated spectra
having 64$k$ data points and a spectral width of 11 ppm, centered at
4.7 ppm, based on the following model function:
\begin{equation}
s(\omega_k) =
 \sum_{p=1}^P
 \frac{A_p \lambda_p}
      {\lambda_p + u_1 (\omega_k - \omega_{0,p})}
\end{equation}
where $s(\omega_k)$ is the $k$-th data point of the spectrum, $P$ equals the
number of peaks, and $u_1$ equals the imaginary unit. Spectra were referenced
and normalized to the DSS peak, and peaks corresponding to HOD and DSS were
subsequently removed, resulting in a basis set of 32 perfectly-phased,
noise-free metabolite spectra. Finally, the basis metabolite spectra were
stored row-wise in a matrix $\mathbf{S}$ for later use in Monte Carlo
calculations.
\end{doublespace}

\begin{figure}[ht!]
\includegraphics[width=6.5in]{figs/pscorr/03-rmse.png}
\caption
      [Monte Carlo Normalization Results.]{
  {\bf Monte Carlo Normalization Results.}
  \\
  Results of 100 Monte Carlo iterations at 0.2$^\circ$ zero-order phase error,
  indicating the ability of all normalization methods to recover the true
  dilution factor of a nearly perfectly phased dataset. Red points reflect the
  dilution factors calculated by integrated the DSS peak and blue points
  reflect the dilution factor estimates from normalization. Upper panels show
  the dilution factors recovered from automatically phased data after
  normalization, and lower panels show dilution factors recovered from
  unphased data after normalization.
}
\end{figure}

\subsubsection{Monte Carlo Experiments}

\begin{doublespace}
Using the basis metabolite spectra, a dataset of 48 simulated metabolomics
spectra ($\mathbf{X} \in \mathbb{H}_1^{N \times K}$) was generated according
to the following equation:
\begin{equation}
\mathbf{X}
 = \mathbf{A} \left( \mathbf{C} \mathbf{S} + \mathbf{1} \mathbf{r}^T \right)
 + \mathbf{E}
\end{equation}
where $\mathbf{A} \in \mathbb{R}^{N \times N}$ is a diagonal matrix of dilution
factors $\alpha_n$, $\mathbf{C} \in \mathbb{R}^{N \times P}$ is a matrix of
metabolite concentrations, $\mathbf{S} \in \mathbb{H}_1^{P \times K}$ is the
previously created metabolite basis set, $\mathbf{r} \in \mathbb{H}_1^K$ is a
spectrum of the DSS reference peak, $\mathbf{1} \in \mathbb{R}^N$ is a vector
of ones, and $\mathbf{E} \in \mathbb{H}_1^{N \times K}$ is a matrix of complex
Gaussian white noise. Dilution factors were drawn from a log-normal
distribution having zero mean and $\sigma = 0.25$. Concentrations in
$\mathbf{C}$ were drawn from normal distributions with parameters chosen to
mimic those in Torgrip et al. (Table 6.2) \cite{torgrip:metab2008}. The
resultant data in $\mathbf{X}$ is a simulated set of $N = 48$ metabolite
extracts, spiked with 100 $\mu$M DSS, where six distinct classes arise from
differences in the concentrations of alanine, asparagine, glutamate, malate,
proline, sucrose and valine. All other metabolites were assigned concentrations
from a normal distribution having $\mu = 5$ $\mu$M and $\sigma = 0.5$ $\mu$M.
\end{doublespace}

\begin{table}[h!]
\caption{Metabolite Concentrations Altered in Monte Carlo Simulations.}
\begin{center}
\begin{tabular}{l | l l l l l l}
  \hline
  {\bf Metabolite} & $\mathbf{C_A}$ ($\mu$M) & $\mathbf{C_B}$ ($\mu$M) &
                     $\mathbf{C_C}$ ($\mu$M) & $\mathbf{C_D}$ ($\mu$M) &
                     $\mathbf{C_E}$ ($\mu$M) & $\mathbf{C_F}$ ($\mu$M) \\
  \hline
  Alanine    &  $9.2 \pm 1.4$    &  $19.6 \pm 1.6$    &  $16.9 \pm 1.2$  &
                $6.5 \pm 0.66$   &  $26.2 \pm 3.6$    &  $13.5 \pm 1.1$  \\
  Asparagine &  $6.8 \pm 0.86$   &  $11.7 \pm 1.8$    &  $19.0 \pm 1.9$  &
               $14.7 \pm 1.2$    &  $24.8 \pm 2.6$    &  $17.4 \pm 1.0$  \\
  Glutamate  & $13.3 \pm 1.7$    &   $9.2 \pm 1.5$    &  $18.8 \pm 1.9$  &
               $16.9 \pm 2.1$    &  $25.0 \pm 3.5$    &   $6.9 \pm 1.0$  \\
  Malate     & $14.2 \pm 1.2$    &  $11.9 \pm 1.4$    &  $22.0 \pm 5.1$  &
                $6.7 \pm 0.68$   &   $9.4 \pm 0.72$   &  $18.0 \pm 2.4$  \\
  Proline    & $11.4 \pm 1.5$    &  $18.4 \pm 3.1$    &  $14.7 \pm 2.4$  &
                $6.9 \pm 0.62$   &   $9.8 \pm 1.5$    &  $23.7 \pm 2.9$  \\
  Sucrose    &  $7.1 \pm 0.9$    &  $17.2 \pm 2.1$    &  $19.3 \pm 2.0$  &
               $13.2 \pm 1.9$    &   $9.3 \pm 0.56$   &  $23.3 \pm 2.7$  \\
  Valine     &  $9.0 \pm 0.85$   &  $26.3 \pm 2.3$    &  $13.4 \pm 1.2$  &
               $20.4 \pm 1.7$    &   $6.7 \pm 0.90$   &  $17.0 \pm 1.5$  \\
\end{tabular}
\end{center}
\end{table}

\begin{doublespace}
Monte Carlo simulations were run to assess the performance of all discussed
normalization methods over various amounts of phase error added to
$\mathbf{X}$. Forty-six phase error points were calculated, in which the
standard deviation of $\theta_0$ was linearly increased from $0^\circ$ to
$5^\circ$. The standard deviation of $\theta_1$ at each point was equal to
one tenth that of $\theta_0$. Both $\theta_0$ and $\theta_1$ were assigned
zero mean. For each phase error point, 100 Monte Carlo iterations were
performed with different sets of random dilution factors. Spectra in the
de-phased $\mathbf{X}$ matrix were automatically phase corrected using simplex
entropy minimization and normalized each time using CS, PQ, HM, SNV, MSC and
PSC methods. Normalization to unit DSS integral was also performed for
reference. An identical set of normalization calculations was performed on the
unphased data. Estimated dilution factors were compared to the true values
to produce a root-mean-square dilution error, $RMSE(\alpha)$, for each method.
Figure 6.3 shows the $RMSE(\alpha)$ result of Monte Carlo simulation at
$0.2^\circ$ phase error. To assess normalization effects on multivariate model
quality, spectra from each method were uniformly binned with 0.04 ppm bin
widths, each bin scaled to unit variance, and subjected to PCA. Values of
$J_2$ for each of the six classes were then calculated, and the median of the
values was reported for each Monte Carlo iteration. The $\varphi$ values
between automatically phased and unphased principal component loadings were
also calculated at each iteration to asses each normalization method's ability
to produce consistent models in the presence of phase errors. Figure 6.4
summarizes the results of Monte Carlo simulation over all phase errors based
on $RMSE(\alpha)$, $J_2$ and $\varphi$.
\end{doublespace}

\begin{figure}[ht!]
\includegraphics[width=6.5in]{figs/pscorr/04-montecarlo.png}
\caption
      [Summary of Monte Carlo Simulation Results.]{
  {\bf Summary of Monte Carlo Simulation Results.}
  \\
  Results of the Monte Carlo simulation over all phase error points.
  ({\bf A}) As phase error increases, dilution factor estimates from all
  methods except PQ remain fairly stable. Estimates from PSC compete with
  MSC, but suffer in comparison with HM.
  ({\bf B}) However, $J_2$ values indicate that PSC outperforms all other
  normalization methods at producing tight clusters at any realistic
  phase error.
  ({\bf C}) Finally, values of $\varphi$ calculated from PCA loadings indicate
  that PSC maintains the highest model consistency in the face of imperfectly
  phased data. Phase error on the $x$-axis refers to zero-order error; it
  should be noted that each point also contains first-order phase error as
  discussed in the Methods.
}
\end{figure}

\subsection{High-throughput Screening}

\subsubsection{Sample Preparation and NMR Acquisition}

\begin{doublespace}
A set of 117 samples containing free ligand mixtures and a set of 117 samples
containing Bovine Serum Albumin (BSA) with ligand mixtures were prepared based
on previously published procedures \cite{powers:ddt2008,mercier:cchts2009}.
In summary, each mixture contained no more than four ligands, each ligand had
a concentration of 100 $\mu$M, and BSA had a concentration of 200 $\mu$M when
present. All NMR samples were prepared to 600 $\mu$L total volume in a buffer
containing 10 mM bis-tris-d$_{19}$, 1.0 mM NaCl, 1.0 mM KCl, 1.0 mM MgCl$_2$
and 10 $\mu$M trimethylsilyl propanoic acid (TMSP) in D$_2$O at pH 7.0
(uncorrected). Samples were loaded into standard 5 mm NMR tubes for spectral
acquisition.
\\\\
All NMR experiments were collected on a Bruker Avance DRX 500 MHz spectrometer
equipped with a 5 mm inverse triple-resonance (\hnmr{}, \cnmr{}, \nnmr{})
cryoprobe with a $z$-axis gradient. A Bruker BACS-120 sample changer and
ICON-NMR software were used to automate NMR data collection. Standard 1D
\hnmr{} NMR spectra were collected for each sample using a SOGGY water
suppression pulse sequence \cite{hwang:jmr1995,nguyen:jmr2007}. All
experiments were performed at 20 $^\circ$C with 256 scans, 8 dummy scans,
a carrier frequency offset of 2352.1 Hz, a 5482.5 Hz spectral width, and a 1.0
section inter-scan delay. Free induction decays were collected with 4$k$
complex data points, resulting in a total acquisition time of 8 minutes per
experiment.
\end{doublespace}

\subsubsection{NMR Data Processing}

\begin{doublespace}
Acquired NMR spectra were loaded and processed in batch inside the GNU Octave
3.6 programming environment \cite{eaton2008} using functions available in the
MVAPACK software package \cite{worley:acscb2014}. Free induction decays were
loaded in from Bruker DMX binary format and corrected for group delay by a
fixed circular shift. All decays were then zero-filled twice, Fourier
transformed and automatically phase corrected using a simplex optimization
routine. Phase-scatter correction was applied to a copy of the screen spectral
data, and spectral remodeling was performed in parallel on the uncorrected and
corrected datasets for the purposes of comparison.
\end{doublespace}

\begin{SCfigure}
\includegraphics[width=3.5in]{figs/pscorr/05-baseline.png}
\caption
      [Statistical Baseline from the BSA Screening Dataset.]{
  {\bf Statistical Baseline from the BSA Screening Dataset.}
  \\
  Statistical baseline ($\boldsymbol{\mu} \pm 4 \boldsymbol{\sigma}$) computed
  from the \hnmr{} NMR ligand-based screen against BSA. The mean baseline is
  traced in deep red, while the confidence region for the baseline is filled
  in light red underneath.
}
\end{SCfigure}

\subsubsection{Statistical Spectral Remodeling}

\begin{doublespace}
The Uncomplicated Statistical Spectral Remodeling (USSR) method capitalizes on
the reproducibility of the protein baseline and the low likelihood that ligand
signals will dominate any given spectral data point across multiple samples.
For each pair of free mixture ($\mathbf{f}_n$) and screen (mixture plus
protein, $\mathbf{p}_n$) \hnmr{} NMR spectra, a difference spectrum
($\mathbf{d}_n$) was computed using a simple point-wise subtraction. The
central tendency ($\boldsymbol{\mu}$) and dispersion ($\boldsymbol{\sigma}$)
of the difference spectra were then robustly estimated using the median and
median absolute deviation, respectively. Figure 6.5 shows the statistical
baseline computed by USSR from a screen of ligand binding to BSA. Once a
statistical baseline is established for a given dataset, each spectrum
$\mathbf{p}_n$ in the screen is remodeled to maximally remove interference
from baseline signals. Each spectral data point in $\mathbf{p}_n$ is compared
to $\boldsymbol{\mu} \pm \boldsymbol{\sigma}$ using a Bonferroni-corrected
Student's $t$-test \cite{dunn:jasa1961}. The resulting $p$ value provides a
measure of how distinguishable the corresponding data point is from the
statistical baseline. Based on a preselected level of significance ($\alpha$),
data points having low $p$ values are retained (less the statistical baseline)
in the remodeled spectrum ($\mathbf{r}_n$) and data points having high $p$
values are modeled as Gaussian white noise. Figure 6.6 shows an example
remodeled spectrum from the ligand binding analysis of BSA.
\end{doublespace}

\begin{figure}[ht!]
\includegraphics[width=6.5in]{figs/pscorr/06-ussrfit.png}
\caption
      [Statistical Baseline Removal from a Screen Spectrum.]{
  {\bf Statistical Baseline Removal from a Screen Spectrum.}
  \\
  An example spectral remodeling result of tolazamide, dimethyl
  4-methoxyisophthalate, 1,7-dimethylxanthine and oxolinic acid in the
  presence of BSA, showing ({\bf A}) the free ligand spectrum (black) and
  the remodeled spectrum (red) resulting from removing the statistical
  baseline (red) from the screen spectrum (black) in ({\bf B}). The remodeled
  pseudospectrum readily indicates that several peaks from dimethyl
  4-methoxyisophthalate have broadened into the baseline due to interaction
  with BSA.
}
\end{figure}

\subsubsection{Statistical Hit Determination}

\begin{doublespace}
For each peak in each remodeled spectrum from USSR, a $K_D$ was computed based
on the intensity ratio between free and remodeled ligand signals. First, in the
limit of fast exchange between free and bound ligand states relative to the NMR
timescale, the fraction of bound ligand ($f_B$) was computed:
\begin{equation}
f_B =
 \left( \frac{I_F}{I_B} - 1 \right)
 \left( \frac{v_B}{v_F} - 1 \right)^{-1}
\end{equation}
where $I_F$ and $I_B$ are the intensities of free and remodeled (bound) ligand
signals, and $v_F$ and $v_B$ are the estimated NMR line widths of the free and
remodeled ligand signals, respectively \cite{shortridge:jcomb2008}. This
fast-exchange assumption may be safely regarded as valid in most
high-throughput 1D \hnmr{} NMR protein-ligand affinity screening experiments
\cite{lepre:chemrev2004}, where the width and intensity of each ligand signal
is a population-weighted sum of its values in the free and bound states.
Without any assumptions about relative concentrations of ligand and protein,
the fraction of bound ligand is related to the total protein concentration
$[P]_T$, total ligand concentration $[L]_T$ and $K_D$ via the following
equation \cite{shortridge:jcomb2008}:
\begin{equation}
f_B = \left[
  1 + \frac{2 K_D}{
    ([P]_T - [L]_T - K_D) + \sqrt{([P]_T - [L]_T - K_D)^2 + 4 K_D [P]_T}
  }
\right]^{-1}
\end{equation}

The solution of the above equation for $K_D$ yields the following result:
\begin{equation}
K_D = \frac{(f_B - 1)(f_B [L]_T - [P]_T)}{f_B}
\end{equation}
which was used to computed per-peak $K_D$ values for each remodeled spectrum
$\mathbf{r}_n$. Finally, the per-peak $K_D$ values were used to compute sample
mean and standard deviation $K_D$ values for each ligand. Hit detection was
accomplished by comparing per-ligand mean and standard deviation $K_D$ values
against a threshold via a Student's $t$-test, where a resulting $p$ value less
than a predefined significant $p$ value was reported as binding.
\end{doublespace}

\begin{figure}[ht!]
\includegraphics[width=6.5in]{figs/pscorr/07-ussrfail.png}
\caption
      [Failed Baseline Removal due to Phase Errors.]{
  {\bf Failed Baseline Removal due to Phase Errors.}
  \\
  Example of a failed USSR result, highlighting the impact of phase error
  during computation and subtraction of the statistical baseline from a screen
  spectrum. Remodeled peaks ({\bf A}, red) upfield of 4.0 ppm are in fact not
  true signals, but were generated due to a phase-induced discrepancy between
  the statistical baseline ({\bf B}, red) and the screen spectrum
  ({\bf B}, black).
}
\end{figure}

\subsubsection{Analysis of Dataset Size}

\begin{doublespace}
A small simulation study was conducted to assess the quality of USSR
statistical baseline estimates over a range of sample sizes (number of spectral
pairs). For sizes from 2 to 116, the BSA dataset was randomly subsampled,
without replacement, to produce a smaller dataset. For each resultant dataset,
the statistical baseline was estimated, and its Pearson correlation to the true
statistical baseline was computed and stored. Over all numbers of spectral
pairs in the simulation, the median baseline correlations were computed, and
are reported in Figure 6.8.
\end{doublespace}

\begin{SCfigure}
\includegraphics[width=3.5in]{figs/pscorr/08-blcorr.png}
\caption
      [Impact of Dataset Size on USSR Statistical Baselines.]{
  {\bf Impact of Dataset Size on USSR Statistical Baselines.}
  \\
  Correlation between statistical baselines from bootstrap-subsampled datasets
  of varying size and the original statistical baseline computed from the
  complete BSA dataset. Lines indicate median correlations, and shaded regions
  indicate confidence regions of plus or minus one standard deviation,
  estimated using median absolute deviation. Blue lines and shaded regions
  indicate values from subsampling the PSC normalized dataset, and red lines
  and shaded regions indicate values from subsampling the uncorrected dataset.
}
\end{SCfigure}

\section{Results}

\begin{doublespace}
FIXME.
\end{doublespace}

\section{Discussion}

\begin{doublespace}
FIXME.
\end{doublespace}

\section{Conclusions}

\begin{doublespace}
FIXME.
\end{doublespace}

\bibliographystyle{abbrv}
\bibliography{bworley}

